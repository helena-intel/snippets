name: Snippet tests

on:
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

jobs:
  build:
    strategy:
      fail-fast: false
      matrix:
        os: ["ubuntu-24.04", "windows-latest"]

    runs-on: ${{ matrix.os }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: 'pip'
          cache-dependency-path: |
            **/requirements*.txt
      - run: |
          pip install -r .github/workflows/requirements_test.txt --extra-index-url https://download.pytorch.org/whl/cpu

      - name: Test snippets with release
        run: |
          pytest .
        env:
          CI: "True"

      - name: Install nightly
        run: |
          pip install --pre --upgrade openvino openvino-tokenizers openvino-genai --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly

      - name: Test snippets with nightly
        run: |
          pytest .
        env:
          CI: "True"

      - name: Download genai and build chat sample
        if: runner.os == 'Windows'
        run: |
          python utils/download_nightly.py
          Get-ChildItem -Path $HOME -Filter setupvars.ps1 -Recurse -File | Select-Object -First 1 | ForEach-Object { & $_.FullName }
          cd llm_chat/cpp
          mkdir build
          cd build
          cmake ..
          cmake --build . --config Release
          .\Release\llm_chat.exe

